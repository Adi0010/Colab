{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jax.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQ0/K25hUihYJzLngvX6fM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adi0010/Colab/blob/master/Jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kj8h6E1eXQb",
        "outputId": "b92dadc3-b207-45d8-89fc-62651c210da8"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as onp\n",
        "import jax.numpy as np\n",
        "from jax import grad, jit, vmap, value_and_grad\n",
        "from jax import random\n",
        "\n",
        "# Generate key which is used to generate random numbers\n",
        "key = random.PRNGKey(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkF2Xw4Zea1r",
        "outputId": "397db9b8-cd4d-4dc8-9c1a-cb33cd5b5852"
      },
      "source": [
        "# Generate a random matrix\n",
        "x = random.uniform(key, (1000, 1000))\n",
        "# Compare running times of 3 different matrix multiplications\n",
        "%time y = onp.dot(x, x)\n",
        "%time y = np.dot(x, x)\n",
        "%time y = np.dot(x, x).block_until_ready()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 45.5 ms, sys: 18.2 ms, total: 63.6 ms\n",
            "Wall time: 38.3 ms\n",
            "CPU times: user 13.2 ms, sys: 11.1 ms, total: 24.3 ms\n",
            "Wall time: 12.2 ms\n",
            "CPU times: user 188 ms, sys: 103 ms, total: 291 ms\n",
            "Wall time: 163 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYU741zsedu5"
      },
      "source": [
        "def ReLU(x):\n",
        "    \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "jit_ReLU = jit(ReLU)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO_rmbEEegCF",
        "outputId": "887741fc-52d8-4547-8c9a-f6e24516f02a"
      },
      "source": [
        "%time out = ReLU(x).block_until_ready()\n",
        "# Call jitted version to compile for evaluation time!\n",
        "%time jit_ReLU(x).block_until_ready()\n",
        "%time out = jit_ReLU(x).block_until_ready()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 48.7 ms, sys: 0 ns, total: 48.7 ms\n",
            "Wall time: 52.3 ms\n",
            "CPU times: user 23.1 ms, sys: 0 ns, total: 23.1 ms\n",
            "Wall time: 22.4 ms\n",
            "CPU times: user 2.32 ms, sys: 100 Âµs, total: 2.42 ms\n",
            "Wall time: 1.54 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABzMLZh_eiDo",
        "outputId": "65e8c9b6-5fe1-42b7-d428-6ec81b01d831"
      },
      "source": [
        "def FiniteDiffGrad(x):\n",
        "    \"\"\" Compute the finite difference derivative approx for the ReLU\"\"\"\n",
        "    return np.array((ReLU(x + 1e-3) - ReLU(x - 1e-3)) / (2 * 1e-3))\n",
        "\n",
        "# Compare the Jax gradient with a finite difference approximation\n",
        "print(\"Jax Grad: \", jit(grad(jit(ReLU)))(2.))\n",
        "print(\"FD Gradient:\", FiniteDiffGrad(2.))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jax Grad:  1.0\n",
            "FD Gradient: 0.99998707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_ZvE8ckek7o"
      },
      "source": [
        "batch_dim = 32\n",
        "feature_dim = 100\n",
        "hidden_dim = 512\n",
        "\n",
        "# Generate a batch of vectors to process\n",
        "X = random.normal(key, (batch_dim, feature_dim))\n",
        "\n",
        "# Generate Gaussian weights and biases\n",
        "params = [random.normal(key, (hidden_dim, feature_dim)),\n",
        "          random.normal(key, (hidden_dim, ))] \n",
        "\n",
        "def relu_layer(params, x):\n",
        "    \"\"\" Simple ReLu layer for single sample \"\"\"\n",
        "    return ReLU(np.dot(params[0], x) + params[1])\n",
        "\n",
        "def batch_version_relu_layer(params, x):\n",
        "    \"\"\" Error prone batch version \"\"\"\n",
        "    return ReLU(np.dot(X, params[0].T) + params[1])\n",
        "\n",
        "def vmap_relu_layer(params, x):\n",
        "    \"\"\" vmap version of the ReLU layer \"\"\"\n",
        "    return jit(vmap(relu_layer, in_axes=(None, 0), out_axes=0))\n",
        "\n",
        "out = np.stack([relu_layer(params, X[i, :]) for i in range(X.shape[0])])\n",
        "out = batch_version_relu_layer(params, X)\n",
        "out = vmap_relu_layer(params, X)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tAQ_6Rkenb6"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFnklKjmerLZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}